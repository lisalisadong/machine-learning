Week 6
---------------------------------------
X. Advice for Applying Machine Learning
---------------------------------------
Debugging a learning algorithm:
-------------------------------
- Get more training examples
- Try smaller set of features
- Try getting additional features
- Try adding polynomial features (x1^2, x2^2, x1x2, etc.)
- Try decreasing lambda
- Try increasing lambda

Diagnostic:
-----------
A test that you can run to gain insight what is/isn't working with a learning
algorithm, and gain guidance as to how best to improve its performance.

Evaluation of learning algorithm:
---------------------------------
Split the dataset -> 70% as training sets, 30 as test sets
Compute test error

Model selection:
----------------
Split the dataset -> 60% as training set, 20% as cross validation set, 20% as testset
use cross validation error to select model
use test error to evaluate

Diagnosing bias vs. variance:
-----------------------------
High bias => underfit: J_training(theta) will be high, J_cv(theta) also high
High variance => overfit: J_training(theta) will be low, J_cv(theta) will be high

Regularization and bias/variance:
---------------------------------
large lambda => bias
small lambda => variance

Learning curves:
----------------
- If a learning algorithm is suffering from high bias, getting more training data
will not (by itself) help much
- If a learning algorithm is suffering form high variance, getting more traning
data is likely to help

Deciding what to do:
--------------------
- Get more training examples => fixes high variance
- Try smaller set of features => fixes high variance
- Try getting additional features => fixes high bias
- Try adding polynomial features (x1^2, x2^2, x1x2, etc.) => fixes high bias
- Try decreasing lambda => fixes high bias
- Try increasing lambda => fixes high variance

---------------------------------------
XI. Machine Learning System Design
---------------------------------------
Precision: True positive / (true positive + false positive)
Recall: True positive / (true positive + false negative)

Trading off precision and recall:
---------------------------------
Predict 1 if y > threshold
larger threshold => higher precision, lower recall
smaller threshold => higher recall, lower precision

F score:
--------
How to compare precision(P)/recall(R) numbers
F1 score = 2PR / (P+R)

Large data rationale:
---------------------
Use a learning algorithm with many parameters (e.g. logistic regression/linear
regression with many features; neural network with many hidden units).
=> low bias
Use a very large training set (unlikely to overfit)
=> low variance
