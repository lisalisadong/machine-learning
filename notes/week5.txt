Week 5
---------------------------------------
IX. Neural Networks: Learning
---------------------------------------
Neural Network Cost Function:
J(Ө) = -(1/m) * Σ(i=1~m)Σ(k=1~K)[y_klog(h(x))_k + (1-y_k)log(1-(h(x)))_k] +
        (λ/2m)Σ(l=1~L-1)Σ(i=1~sl)Σ(j=1~sl+1)(Ө_j_i)^2

Gradient Computation: Back-propagation algorithm:
-------------------------------------------------
δ_j_l = error of node j in layer l
δ_j_4 = a_j_4 - y_j
=> vectorized =>
δ_4 = a_4 - y
δ_3 = (Ө_3)^T * δ_4 .* g'(z_3) ---- g'(z_3) = a_3 .* (1-a_3)
δ_2 = (Ө_2)^T * δ_3 .* g'(z_2) ---- g'(z_2) = a_2 .* (1-a_2)
(no δ_1)

set Δ_ij_l = 0 (for all l, i, j)
for i = 0 to m
    set a_1 = x_i
    perform forward propagation to compute a_l for l = 2, 3, ...., L
    Using y_i, compute δ_l = a_L - y
    Compute δ_L-1, δ_l-2,..., δ_2
    Δ_ij_l = Δ_ij_l + a_j_l * δ_i_l+1

D_ij_l = (1/m)Δ_ij_l + λӨ_ij_l if j != 0
D_ij_l = (1/m)Δ_ij_l           if j != 0
<= Δ/ΔӨ * J(Ө) = D_ij_l

Advanced optimization
---------------------
function [jVal, gradient] = costFunction(theta)
...
optTheta = fminunc(@costFunction, initialTheta, options)

Neural'Network'(L=4):
    Ө_1, Ө_2, Ө_3 - matrices (Theta1, Theta2, Theta3)
    D_1, D_2, D_3 - matrices (D1, D2, D3)

“Unroll” into vectors - Example:
s1 = 10, s2= 10, s3 = 1
Ө_1 is 10x11 matrix, Ө_2 is 10x11 matrix, Ө_3 is 1x11matrix
D_1 is 10x11 matrix, D_2 is 10x11 matrix, D_3 is 1x11matrix

thetaVec = [Theta1(:); Theta2(:); Theta3(:)];
DVec = [D1(:); D2(:); D3(:)];
Theta1 = reshape(thetaVec(1:110),10,11);
Theta2 = reshape(thetaVec(111:220),10,11);
Theta3 = reshape(thetaVec(221:231),1,11);

Learning Algorithm:
Have initial parameters Ө_1, Ө_2, Ө_3.
Unroll to get initialTheta to pass to
fminunc(@costFunction, initialTheta, options)

function [jval, gradientVec] = costFunction(thetaVec)
From thetaVec, get Ө_1, Ө_2, Ө_3
Use forward prop/back prop to compute D_1, D_2, D_3
Unroll J(Ө) and D_1, D_2, D_3 to get gradientVec

Gradient Checking
-----------------
Implement: gradApprox = (J(theta + EPSILON) – J(theta – EPSILON)) /(2*EPSILON)

for i = 1:n,
    thetaPlus = theta;
    thetaPlus(i) = thetaPlus(i) + EPSILON;
    thetaMinus = theta;
    thetaMinus(i) = thetaMinus(i) – EPSILON;
    gradApprox(i) = (J(thetaPlus) – J(thetaMinus)) /(2*EPSILON);
end;
Check that gradApprox ≈ DVec

Implementation Note:
- Implement backprop to compute DVec (unrolled D_1, D_2, D_3).
- Implement numerical gradient check to compute gradApprox.
- Make sure they give similar values.
- Turn off gradient checking. Using backprop code for learning.
Important:
- Be sure to disable your gradient checking code before training your classifier.
If you run numerical gradient computati on on every iteration of gradient descent
(or in the inner loop of costFunction(…))your code will be very slow.

Initial value of Ө
------------------
For gradient descent and advanced optimization method, need initial value for Ө
optTheta = fminunc(@costFunction, initialTheta, options)

Consider gradient descent
Set initialTheta = zeros(n, 1)?
=> After each update, parameters corresponding to inputs going into each of two
hidden units are identical.
=> won't work

Random initialization: Symmetry breaking
Initialize each Ө_ij_l to a random value in [-ε, ε] (i.e. -ε <= Ө_ij_l <= ε)
E.g.
Theta1 = rand(10,11)*(2*INIT_EPSILON) - INIT_EPSILON;
Theta2 = rand(1,11)*(2*INIT_EPSILON) - INIT_EPSILON;

Training a neural network
-------------------------
Pick a network architecture (connectivity pattern between neurons)
No. of input units: Dimension of features
No. output units: Number of classes
Reasonable default: 1 hidden layer, or if >1 hidden layer, have same no. of hidden
units in every layer (usually the more the better)

1. Randomly initialize weights
2. Implement forward propagation to get h(x_i) for any x_i
3. Implement code to compute cost function
4. Implement backprop to compute partial derivatives Δ/ΔӨ * J(Ө)
    for i = 1:m
        Perform forward propagation and backpropagation using example (x_i. y_i)
        (Get activations a_l and delta terms δ_l for l = 2, ..., L).
5. Use gradient checking to compare Δ/ΔӨ * J(Ө) computed using backpropagation vs.
using numerical estimate of gradient of J(Ө).
Then disable gradient checking code.
6. Use gradient descent or advanced optimization method with backpropagation to
try to minimize J(Ө) as a function of parameters Ө
